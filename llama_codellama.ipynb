{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "deaf1b5b-1735-4d47-b60f-5a568dd8b189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input the agent model API KEY:  ········\n",
      "Input the code model API KEY:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered model is: meta/llama3-8b-instruct\n",
      "Decision by Llama3: 'code'\n",
      "Response generated by: Code Llama\n",
      "Discovered model is: meta-llama/CodeLlama-7b-Instruct-hf\n",
      "  Here is a Python code to retrieve the odd numbers in a list:\n",
      "```\n",
      "def get_odd_numbers(my_list):\n",
      "    return [x for x in my_list if x % 2 != 0]\n",
      "```\n",
      "This code uses a list comprehension to iterate over the elements of the input list `my_list`, and returns a new list containing only the odd numbers. The `if x % 2 != 0` condition checks if the element is odd or not, and the `[x for x in my_list if x % 2 != 0]` syntax creates a new list containing only the elements that satisfy the condition.\n",
      "\n",
      "You can use this function as follows:\n",
      "```\n",
      "my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "odd_numbers = get_odd_numbers(my_list)\n",
      "print(odd_numbers)  # [1, 3, 5, 7, 9]\n",
      "```\n",
      "Discovered model is: meta/llama3-8b-instruct\n",
      "Final response generated by Llama3\n",
      "Here is the Python code to retrieve the odd numbers in a list:\n",
      "```\n",
      "def get_odd_numbers(my_list):\n",
      "    return [x for x in my_list if x % 2 != 0]\n",
      "\n",
      "my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "odd_numbers = get_odd_numbers(my_list)\n",
      "print(odd_numbers)  # [1, 3, 5, 7, 9]\n",
      "```\n",
      "This code defines a function `get_odd_numbers` that takes a list `my_list` as input, and returns a new list containing only the odd numbers from the input list. The function uses a list comprehension to iterate over the elements of the input list, and checks if each element is odd using the condition `x % 2 != 0`. If the element is odd, it is included in the new list. The resulting list of odd numbers is then printed to the console."
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import httpx\n",
    "####to investigate\n",
    "#### decision will be in 2 categories like : code and general, and for general will retrieve from llama3 and the code from codellama\n",
    "from getpass import getpass\n",
    "import sys\n",
    "\n",
    "# https://codellama.130.162.32.162.nip.io/       - helm\n",
    "# https://llama3.130.162.32.162.nip.io/          - helm\n",
    "#http://193.122.2.134:8000/v1/                   - first\n",
    "#http://129.159.199.7.nip.io/v1/                 - first\n",
    "# agent_llm_api_key = \"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\"\n",
    "# code_llm_api_key = \"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\"\n",
    "\n",
    "\n",
    "# Update the BaseURL of the model (we are using the service name)\n",
    "# $ kubectl get service\n",
    "agent_base_url = \"http://agent_llm/v1\"\n",
    "code_base_url = \"http://code_llm/v1\"\n",
    "\n",
    "# agent_base_url = \"https://nimllama3.130.162.32.162.nip.io/v1/\"\n",
    "# code_base_url = \"https://codellama.130.162.32.162.nip.io/v1\"\n",
    "\n",
    "\n",
    "#When using NIM, the llm_api_key is not used, you can provide a dummy value.\n",
    "agent_llm_api_key  = getpass('Input the agent model API KEY: ')\n",
    "if not agent_llm_api_key:\n",
    "    agent_llm_api_key = \"dummy\"\n",
    "\n",
    "# When using NIM, the llm_api_key is not used, you can provide a dummy value.\n",
    "code_llm_api_key  = getpass('Input the code model API KEY: ')\n",
    "if not code_llm_api_key:\n",
    "    code_llm_api_key = \"dummy\"\n",
    "\n",
    "agent_model_config = {\n",
    "    \"base_url\" : agent_base_url,\n",
    "    \"api_key\" : agent_llm_api_key,\n",
    "    \"temperature\" : 0,\n",
    "    \"top_p\" : 1,\n",
    "    \"max_tokens\": 256,\n",
    "    \"stream\" : True\n",
    "    }\n",
    "\n",
    "code_model_config = {\n",
    "    \"base_url\" : code_base_url,\n",
    "    \"api_key\" : code_llm_api_key,\n",
    "    \"temperature\" : 0,\n",
    "    \"top_p\" : 1,\n",
    "    \"max_tokens\" : 256,\n",
    "    \"stream\" : True\n",
    "    }\n",
    "\n",
    "\n",
    "httpx_client = httpx.Client(verify=False)\n",
    "######Modify with client_agent, client_code \n",
    "client_agent = OpenAI(\n",
    "  base_url=agent_model_config[\"base_url\"],\n",
    "  api_key = agent_model_config[\"api_key\"],\n",
    "  http_client=httpx_client\n",
    ")\n",
    "client_code = OpenAI(\n",
    "  base_url= code_model_config[\"base_url\"],\n",
    "  api_key = code_model_config[\"api_key\"],\n",
    "  http_client=httpx_client\n",
    ")\n",
    "def discover_model(client):\n",
    "#discover available models and use the first one\n",
    "    available_models = client.models.list()\n",
    "    if len(available_models.data):\n",
    "        model = available_models.data[0].id\n",
    "        print(f\"Discovered model is: {model}\")\n",
    "    else:\n",
    "        print(\"No model discovered\")\n",
    "        sys.exit(1)\n",
    "    return model\n",
    "\n",
    "def create_completion(client, messages, config):\n",
    "    return client.chat.completions.create(\n",
    "        model=discover_model(client),\n",
    "        messages=messages,\n",
    "        temperature=config[\"temperature\"],\n",
    "        top_p=config[\"top_p\"],\n",
    "        max_tokens=config[\"max_tokens\"],\n",
    "        stream=config[\"stream\"]\n",
    "    )\n",
    "def get_decision_from_agent(query):\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Please classify the following query into one of the three categories: 'code', 'optimization', or 'general'. 'code' for queries asking for code examples, programming syntax, or specific implementation details. 'optimization' for queries related to improving performance, enhancing efficiency, or refining algorithms. 'general' for queries that do not specifically pertain to coding or optimization but instead involve general concepts, explanations, or broad questions. Answer only with 'code', 'optimization', or 'general': {query}\"\n",
    "    }]\n",
    "\n",
    "    response = create_completion(client_agent, messages, {**agent_model_config, \"max_tokens\": 10})\n",
    "    decision = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            decision += chunk.choices[0].delta.content\n",
    "    return decision.strip().lower()\n",
    "\n",
    "def get_general_response_from_agent(query):\n",
    "    messages=[{\"role\":\"user\",\"content\": query}]\n",
    "    response = create_completion(client_agent, messages, agent_model_config)\n",
    "    return response\n",
    "\n",
    "def get_response_from_code_model(query):\n",
    "    messages=[{\"role\":\"user\",\"content\": query}]\n",
    "    response = create_completion(client_code, messages, code_model_config)\n",
    "    codellama_response = \"\"\n",
    "    for chunk in response:\n",
    "      if chunk.choices[0].delta.content is not None:\n",
    "        codellama_response = codellama_response + chunk.choices[0].delta.content\n",
    "    print(codellama_response)\n",
    "    return codellama_response.strip().lower()\n",
    "\n",
    "def get_preliminary_response(query):\n",
    "    decision = get_decision_from_agent(query)\n",
    "    print(f\"Decision by Llama3: '{decision}'\")\n",
    "    \n",
    "    if \"code\" in decision:\n",
    "        print(\"Response generated by: Code Llama\")\n",
    "        response = get_response_from_code_model(query)\n",
    "    elif \"optimization\" in decision:\n",
    "        print(\"Response generated by: cuopt\")\n",
    "        response = get_response_from_optimization(query)\n",
    "    else:\n",
    "        print(\"Response generated by: Llama3\")\n",
    "        response =  get_general_response_from_agent(query)\n",
    "    return response\n",
    "\n",
    "def get_final_response_from_agent_workflow(query, preliminary_response):\n",
    "    messages=[{\"role\": \"user\", \"content\": f\"Use this information {preliminary_response} and generate the response for the question: {query}\"}]\n",
    "    final_response = create_completion(client_agent, messages, agent_model_config)\n",
    "    return final_response\n",
    "\n",
    "\n",
    "\n",
    "query = \"Write a python code to retrieve the odd numbers in a list\"\n",
    "#query = \"Can a python code answer the question how far is the moon ?\" #how to make the agent to retrieve the code from the code model ?\n",
    "preliminary_response = get_preliminary_response(query)\n",
    "final_response = get_final_response_from_agent_workflow(query, preliminary_response)\n",
    "print(\"Final response generated by Llama3\")\n",
    "for chunk in final_response:\n",
    "  if chunk.choices[0].delta.content is not None:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57533f09-404a-449a-a4a5-266f530b1851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
