{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "deaf1b5b-1735-4d47-b60f-5a568dd8b189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Input the agent model API KEY:  ········\n",
      "Input the code model API KEY:  ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovered model is: meta/llama3-8b-instruct\n",
      "Decision by Llama3: 'code'\n",
      "Response generated by: Code Llama\n",
      "Discovered model is: meta-llama/CodeLlama-7b-Instruct-hf\n",
      "  Here is a Python code to retrieve the odd numbers in a list:\n",
      "```\n",
      "def get_odd_numbers(my_list):\n",
      "    return [x for x in my_list if x % 2 != 0]\n",
      "```\n",
      "This code uses a list comprehension to iterate over the elements of the input list `my_list`, and returns a new list containing only the odd numbers. The `if x % 2 != 0` condition checks if the element is odd or not, and the `[x for x in my_list if x % 2 != 0]` syntax creates a new list containing only the elements that satisfy the condition.\n",
      "\n",
      "You can use this function as follows:\n",
      "```\n",
      "my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "odd_numbers = get_odd_numbers(my_list)\n",
      "print(odd_numbers)  # [1, 3, 5, 7, 9]\n",
      "```\n",
      "Discovered model is: meta/llama3-8b-instruct\n",
      "Final response generated by Llama3\n",
      "Here is the Python code to retrieve the odd numbers in a list:\n",
      "```\n",
      "def get_odd_numbers(my_list):\n",
      "    return [x for x in my_list if x % 2 != 0]\n",
      "\n",
      "my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "odd_numbers = get_odd_numbers(my_list)\n",
      "print(odd_numbers)  # [1, 3, 5, 7, 9]\n",
      "```\n",
      "This code defines a function `get_odd_numbers` that takes a list `my_list` as input, and returns a new list containing only the odd numbers from the input list. The function uses a list comprehension to iterate over the elements of the input list, and checks if each element is odd using the condition `x % 2 != 0`. If the element is odd, it is included in the new list. The resulting list of odd numbers is then printed to the console."
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from getpass import getpass\n",
    "import sys\n",
    "\n",
    "# Update the BaseURL of the model (we are using the service name)\n",
    "# $ kubectl get service\n",
    "agent_base_url = \"http://llama3/v1\"\n",
    "code_base_url = \"http://codellama/v1\"\n",
    "\n",
    "#Adding api_key to secure the models.\n",
    "#When using NIM, the llm_api_key is not used, you can provide a dummy value.\n",
    "agent_llm_api_key  = getpass('Input the agent model API KEY: ')\n",
    "if not agent_llm_api_key:\n",
    "    agent_llm_api_key = \"dummy\"\n",
    "\n",
    "code_llm_api_key  = getpass('Input the code model API KEY: ')\n",
    "if not code_llm_api_key:\n",
    "    code_llm_api_key = \"dummy\"\n",
    "\n",
    "#Define parameters for different models\n",
    "agent_model_config = {\n",
    "    \"base_url\" : agent_base_url,\n",
    "    \"api_key\" : agent_llm_api_key,\n",
    "    \"temperature\" : 0,\n",
    "    \"top_p\" : 1,\n",
    "    \"max_tokens\": 256,\n",
    "    \"stream\" : True\n",
    "    }\n",
    "\n",
    "code_model_config = {\n",
    "    \"base_url\" : code_base_url,\n",
    "    \"api_key\" : code_llm_api_key,\n",
    "    \"temperature\" : 0,\n",
    "    \"top_p\" : 1,\n",
    "    \"max_tokens\" : 256,\n",
    "    \"stream\" : True\n",
    "    }\n",
    "\n",
    "#Define clients\n",
    "client_agent = OpenAI(\n",
    "  base_url=agent_model_config[\"base_url\"],\n",
    "  api_key = agent_model_config[\"api_key\"]\n",
    ")\n",
    "client_code = OpenAI(\n",
    "  base_url= code_model_config[\"base_url\"],\n",
    "  api_key = code_model_config[\"api_key\"]\n",
    ")\n",
    "\n",
    "#Discover available models and use the first one\n",
    "def discover_model(client):\n",
    "    available_models = client.models.list()\n",
    "    if len(available_models.data):\n",
    "        model = available_models.data[0].id\n",
    "        print(f\"Discovered model is: {model}\")\n",
    "    else:\n",
    "        print(\"No model discovered\")\n",
    "        sys.exit(1)\n",
    "    return model\n",
    "\n",
    "#Define completition of model\n",
    "def create_completion(client, messages, config):\n",
    "    return client.chat.completions.create(\n",
    "        model=discover_model(client),\n",
    "        messages=messages,\n",
    "        temperature=config[\"temperature\"],\n",
    "        top_p=config[\"top_p\"],\n",
    "        max_tokens=config[\"max_tokens\"],\n",
    "        stream=config[\"stream\"]\n",
    "    )\n",
    "\n",
    "#Get decision from agent model if the query is related to code, optimization or general\n",
    "def get_decision_from_agent(query):\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Please classify the following query into one of the three categories: 'code', 'optimization', or 'general'. 'code' for queries asking for code examples, programming syntax, or specific implementation details. 'optimization' for queries related to improving performance, enhancing efficiency, or refining algorithms. 'general' for queries that do not specifically pertain to coding or optimization but instead involve general concepts, explanations, or broad questions. Answer only with 'code', 'optimization', or 'general': {query}\"\n",
    "    }]\n",
    "\n",
    "    response = create_completion(client_agent, messages, {**agent_model_config, \"max_tokens\": 10})\n",
    "    decision = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            decision += chunk.choices[0].delta.content\n",
    "    return decision.strip().lower()\n",
    "\n",
    "# Get response from the agent\n",
    "def get_general_response_from_agent(query):\n",
    "    messages=[{\"role\":\"user\",\"content\": query}]\n",
    "    response = create_completion(client_agent, messages, agent_model_config)\n",
    "    agent_response = \"\"\n",
    "    for chunk in response:\n",
    "      if chunk.choices[0].delta.content is not None:\n",
    "        agent_response = agent_response + chunk.choices[0].delta.content\n",
    "    print(agent_response)\n",
    "    return agent_response\n",
    "\n",
    "# Get response from code model\n",
    "def get_response_from_code_model(query):\n",
    "    messages=[{\"role\":\"user\",\"content\": query}]\n",
    "    response = create_completion(client_code, messages, code_model_config)\n",
    "    codemodel_response = \"\"\n",
    "    for chunk in response:\n",
    "      if chunk.choices[0].delta.content is not None:\n",
    "        codemodel_response = codemodel_response + chunk.choices[0].delta.content\n",
    "    print(codemodel_response)\n",
    "    return codemodel_response.strip().lower()\n",
    "\n",
    "\n",
    "# Get response from the designated model based on code, optimisation or general\n",
    "def get_preliminary_response(query):\n",
    "    decision = get_decision_from_agent(query)\n",
    "    print(f\"Decision by Agent Workflow: '{decision}'\")\n",
    "    \n",
    "    if \"code\" in decision:\n",
    "        print(\"Response generated by: Code Agent\")\n",
    "        response = get_response_from_code_model(query)\n",
    "    elif \"optimization\" in decision:\n",
    "        print(\"Response generated by: Optimization Agent\")\n",
    "        response = get_response_from_optimization(query)\n",
    "    else:\n",
    "        print(\"Response generated by: Agent\")\n",
    "        response =  get_general_response_from_agent(query)\n",
    "    return response\n",
    "\n",
    "# Get the final response from the agent model using the information that was recieved from other models.\n",
    "def get_final_response_from_agent_workflow(query, preliminary_response):\n",
    "    messages=[{\"role\": \"user\", \"content\": f\"Use this information {preliminary_response} and generate the response for the question: {query}\"}]\n",
    "    final_response = create_completion(client_agent, messages, agent_model_config)\n",
    "    return final_response\n",
    "\n",
    "\n",
    "#query examples\n",
    "query = \"Write a python code to retrieve the odd numbers in a list\"\n",
    "# query = \"What is the distance between earth and moon ?\"\n",
    "preliminary_response = get_preliminary_response(query)\n",
    "final_response = get_final_response_from_agent_workflow(query, preliminary_response)\n",
    "print(\"Final response generated by Agent\")\n",
    "for chunk in final_response:\n",
    "  if chunk.choices[0].delta.content is not None:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57533f09-404a-449a-a4a5-266f530b1851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
