{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install openai\n",
    "!python3 -m pip install gradio\n",
    "!python3 -m pip install --upgrade jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from openai import OpenAI\n",
    "import sys\n",
    "\n",
    "# Update the BaseURL of the model (we are using the service name)\n",
    "# $ kubectl get service\n",
    "agent_base_url = \"http://llama3/v1\"\n",
    "code_base_url = \"http://codellama/v1\"\n",
    "\n",
    "# Define parameters for different models\n",
    "agent_model_config = {\n",
    "    \"base_url\": agent_base_url,\n",
    "    \"api_key\": \"dummy\",\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 1,\n",
    "    \"max_tokens\": 256,\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "code_model_config = {\n",
    "    \"base_url\": code_base_url,\n",
    "    \"api_key\": \"dummy\",\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 1,\n",
    "    \"max_tokens\": 256,\n",
    "    \"stream\": True\n",
    "}\n",
    "\n",
    "# Cache for discovered models\n",
    "cached_models = {}\n",
    "\n",
    "def discover_model(client):\n",
    "    if client in cached_models:\n",
    "        return cached_models[client]\n",
    "    \n",
    "    available_models = client.models.list()\n",
    "    if len(available_models.data):\n",
    "        model = available_models.data[0].id\n",
    "        print(f\"Discovered model is: {model}\")\n",
    "        cached_models[client] = model\n",
    "    else:\n",
    "        print(\"No model discovered\")\n",
    "        sys.exit(1)\n",
    "    return model\n",
    "\n",
    "def create_completion(client, messages, config):\n",
    "    model = discover_model(client)\n",
    "    if model is None:\n",
    "        return []\n",
    "    try:\n",
    "        return client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            temperature=config[\"temperature\"],\n",
    "            top_p=config[\"top_p\"],\n",
    "            max_tokens=config[\"max_tokens\"],\n",
    "            stream=config[\"stream\"]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Error: \", e)\n",
    "        return []\n",
    "\n",
    "def get_decision_from_agent(query):\n",
    "    messages = [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Please classify the following query into one of the three categories: 'code', 'optimization', or 'general'. 'code' for queries asking for code examples, programming syntax, or specific implementation details. 'optimization' for queries related to improving performance, enhancing efficiency, or refining algorithms. 'general' for queries that do not specifically pertain to coding or optimization but instead involve general concepts, explanations, or broad questions. Answer only with 'code', 'optimization', or 'general': {query}\"\n",
    "    }]\n",
    "    response = create_completion(client_agent, messages, {**agent_model_config, \"max_tokens\": 10})\n",
    "    decision = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            decision += chunk.choices[0].delta.content\n",
    "    return decision.strip().lower()\n",
    "\n",
    "def get_general_response_from_agent(query):\n",
    "    messages = [{\"role\": \"user\", \"content\": query}]\n",
    "    response = create_completion(client_agent, messages, agent_model_config)\n",
    "    agent_response = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            agent_response += chunk.choices[0].delta.content\n",
    "    print(agent_response)\n",
    "    return agent_response\n",
    "\n",
    "def get_response_from_code_model(query):\n",
    "    messages = [{\"role\": \"user\", \"content\": query}]\n",
    "    response = create_completion(client_code, messages, code_model_config)\n",
    "    codemodel_response = \"\"\n",
    "    for chunk in response:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            codemodel_response += chunk.choices[0].delta.content\n",
    "    print(codemodel_response)\n",
    "    return codemodel_response.strip().lower()\n",
    "\n",
    "def get_preliminary_response(query):\n",
    "    decision = get_decision_from_agent(query)\n",
    "    log = f\"Decision by Agent Workflow: '{decision}'\\n\"\n",
    "    \n",
    "    if \"code\" in decision:\n",
    "        log += \"Response generated by: Code Agent\\n\"\n",
    "        response = get_response_from_code_model(query)\n",
    "    elif \"optimization\" in decision:\n",
    "        log += \"Response generated by: Optimization Agent\\n\"\n",
    "        response = get_response_from_optimization(query)\n",
    "    else:\n",
    "        log += \"Response generated by: General Agent\\n\"\n",
    "        response = get_general_response_from_agent(query)\n",
    "    \n",
    "    log += f\"Preliminary response:\\n{response}\\n\"\n",
    "    return response, log\n",
    "\n",
    "def get_final_response_from_agent_workflow(query, preliminary_response):\n",
    "    messages = [{\"role\": \"user\", \"content\": f\"Use this information {preliminary_response} and generate the response for the question: {query}\"}]\n",
    "    final_response = create_completion(client_agent, messages, agent_model_config)\n",
    "    final_response_text = \"\"\n",
    "    for chunk in final_response:\n",
    "        if chunk.choices[0].delta.content is not None:\n",
    "            final_response_text += chunk.choices[0].delta.content\n",
    "    return final_response_text\n",
    "\n",
    "def gradio_interface(query):\n",
    "    preliminary_response, log = get_preliminary_response(query)\n",
    "    final_response = get_final_response_from_agent_workflow(query, preliminary_response)\n",
    "    log += f\"Final response:\\n{final_response}\"\n",
    "    return log\n",
    "\n",
    "def set_api_keys(agent_key, code_key):\n",
    "    global agent_model_config, code_model_config\n",
    "    agent_model_config[\"api_key\"] = agent_key\n",
    "    code_model_config[\"api_key\"] = code_key\n",
    "    return agent_model_config, code_model_config\n",
    "\n",
    "def client_init(agent_config, code_config):\n",
    "    client_agent = OpenAI(\n",
    "        base_url=agent_config[\"base_url\"],\n",
    "        api_key=agent_config[\"api_key\"]\n",
    "    )\n",
    "    client_code = OpenAI(\n",
    "        base_url=code_config[\"base_url\"],\n",
    "        api_key=code_config[\"api_key\"]\n",
    "    )\n",
    "    return client_agent, client_code\n",
    "\n",
    "def set_keys_and_init_clients(agent_key, code_key):\n",
    "    agent_config, code_config = set_api_keys(agent_key, code_key)\n",
    "    global client_agent, client_code\n",
    "    client_agent, client_code = client_init(agent_config, code_config)\n",
    "    return \"API keys set and clients initialized.\"\n",
    "\n",
    "api_key_interface = gr.Interface(\n",
    "    fn=set_keys_and_init_clients,\n",
    "    # inputs=[\"text\", \"text\"],\n",
    "    inputs=[gr.Textbox(type=\"password\", placeholder=\"Enter agent API key\"), gr.Textbox(type=\"password\", placeholder=\"Enter code API key\")],\n",
    "    outputs=\"text\",\n",
    "    title=\"Set API Keys\",\n",
    "    description=\"Input the API keys for the agent and code models.\"\n",
    ")\n",
    "workflow_interface = gr.Interface(\n",
    "    fn=gradio_interface,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"Agent Workflow\",\n",
    "    description=\"Enter your query below:\"\n",
    ")\n",
    "\n",
    "gr.TabbedInterface([api_key_interface, workflow_interface], [\"Set API Keys\", \"Agent Workflow\"]).launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
